[
  {
    "objectID": "posts/papers/token_cache_2409_18523/index.html#tokencache",
    "href": "posts/papers/token_cache_2409_18523/index.html#tokencache",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Œ∏} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Œ∏}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Œ∏}?\n\nInstead of predicing binary importance [0,1], use interpolation to ‚Äúsuperpose‚Äù pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} ‚¶ø f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) ‚¶ø f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\n\n\nüîç Optimization Objective\n\n\n\n\n‚Ñí_{\\text{mse}} = ùîº_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [‚Äñ z^{t}_{L+1} - \\hat{z}^{t}_{L+1}‚Äñ^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/index.html#cache-predictor-visualized",
    "href": "posts/papers/token_cache_2409_18523/index.html#cache-predictor-visualized",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\n\n\n\n\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/index.html#bibliography",
    "href": "posts/papers/token_cache_2409_18523/index.html#bibliography",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "üìë Bibliography",
    "text": "üìë Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. ‚ÄúToken Caching for Diffusion Transformer Acceleration.‚Äù https://arxiv.org/abs/2409.18523.\n\n\n\n\n\nFigure¬†1: Overview of TokenCache\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/slides.html#tokencache",
    "href": "posts/papers/token_cache_2409_18523/slides.html#tokencache",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Œ∏} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Œ∏}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Œ∏}?\n\nInstead of predicing binary importance [0,1], use interpolation to ‚Äúsuperpose‚Äù pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} ‚¶ø f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) ‚¶ø f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\nüîç Optimization Objective\n\n\n\n‚Ñí_{\\text{mse}} = ùîº_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [‚Äñ z^{t}_{L+1} - \\hat{z}^{t}_{L+1}‚Äñ^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/slides.html#cache-predictor-visualized",
    "href": "posts/papers/token_cache_2409_18523/slides.html#cache-predictor-visualized",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/slides.html#bibliography",
    "href": "posts/papers/token_cache_2409_18523/slides.html#bibliography",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "üìë Bibliography",
    "text": "üìë Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. ‚ÄúToken Caching for Diffusion Transformer Acceleration.‚Äù https://arxiv.org/abs/2409.18523.\n\n\n\n\n\n\n\n\n\nFigure¬†1: Overview of TokenCache\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/index.html#tokencache",
    "href": "posts/papers/ToCa/index.html#tokencache",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Œ∏} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Œ∏}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Œ∏}?\n\nInstead of predicing binary importance [0,1], use interpolation to ‚Äúsuperpose‚Äù pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} ‚¶ø f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) ‚¶ø f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\n\n\nüîç Optimization Objective\n\n\n\n\n‚Ñí_{\\text{mse}} = ùîº_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [‚Äñ z^{t}_{L+1} - \\hat{z}^{t}_{L+1}‚Äñ^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/ToCa/index.html#cache-predictor-visualized",
    "href": "posts/papers/ToCa/index.html#cache-predictor-visualized",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\n\n\n\n\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/index.html#bibliography",
    "href": "posts/papers/ToCa/index.html#bibliography",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "üìë Bibliography",
    "text": "üìë Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. ‚ÄúToken Caching for Diffusion Transformer Acceleration.‚Äù https://arxiv.org/abs/2409.18523.\n\n\n\n\n\nFigure¬†1: Overview of TokenCache\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/slides.html#tokencache",
    "href": "posts/papers/ToCa/slides.html#tokencache",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Œ∏} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Œ∏}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Œ∏}?\n\nInstead of predicing binary importance [0,1], use interpolation to ‚Äúsuperpose‚Äù pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} ‚¶ø f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) ‚¶ø f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\nüîç Optimization Objective\n\n\n\n‚Ñí_{\\text{mse}} = ùîº_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [‚Äñ z^{t}_{L+1} - \\hat{z}^{t}_{L+1}‚Äñ^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/ToCa/slides.html#cache-predictor-visualized",
    "href": "posts/papers/ToCa/slides.html#cache-predictor-visualized",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/slides.html#bibliography",
    "href": "posts/papers/ToCa/slides.html#bibliography",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "üìë Bibliography",
    "text": "üìë Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. ‚ÄúToken Caching for Diffusion Transformer Acceleration.‚Äù https://arxiv.org/abs/2409.18523.\n\n\n\n\n\n\n\n\n\nFigure¬†1: Overview of TokenCache\nFigure¬†2: Overview of Cache Predictor"
  },
  {
    "objectID": "index.html#about-meinspiration",
    "href": "index.html#about-meinspiration",
    "title": "",
    "section": "üßëüèª‚Äçüíª About Me1",
    "text": "üßëüèª‚Äçüíª About Me1\n\nüíª currently a computer science graduate student @ (UT Austin).\n\n\n\nüöÄ generatly interested in application of AI/ML specifically in domains of Generative AI (Diffusion Models), 3D Vision, & Multi-Modality2\n\n\n\n\n\nWebsite inspiration and code base ported from Sam Foreman‚Äôs Personal Website. Huge thanks to Sam üêê!‚Ü©Ô∏é\nIf this sounds like something you‚Äôd be interested in doing, please feel free to reach out to me!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/index.html#reading-list---arxiv-papers",
    "href": "posts/index.html#reading-list---arxiv-papers",
    "title": "üì¨ Posts",
    "section": "üóûÔ∏èReading List - arXiv papers",
    "text": "üóûÔ∏èReading List - arXiv papers\n\nView Reading List"
  },
  {
    "objectID": "posts/papers/index.html#reading-list---arxiv-papers",
    "href": "posts/papers/index.html#reading-list---arxiv-papers",
    "title": "üì¨ Posts",
    "section": "üöÄ Reading List - arXiv papers",
    "text": "üöÄ Reading List - arXiv papers"
  }
]