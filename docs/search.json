[
  {
    "objectID": "posts/papers/token_cache_2409_18523/index.html#tokencache",
    "href": "posts/papers/token_cache_2409_18523/index.html#tokencache",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Î¸} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Î¸}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Î¸}?\n\nInstead of predicing binary importance [0,1], use interpolation to â€œsuperposeâ€ pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} â¦¿ f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) â¦¿ f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\n\n\nğŸ” Optimization Objective\n\n\n\n\nâ„’_{\\text{mse}} = ğ”¼_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [â€– z^{t}_{L+1} - \\hat{z}^{t}_{L+1}â€–^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/index.html#cache-predictor-visualized",
    "href": "posts/papers/token_cache_2409_18523/index.html#cache-predictor-visualized",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\n\n\n\n\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/index.html#bibliography",
    "href": "posts/papers/token_cache_2409_18523/index.html#bibliography",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "ğŸ“‘ Bibliography",
    "text": "ğŸ“‘ Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. â€œToken Caching for Diffusion Transformer Acceleration.â€ https://arxiv.org/abs/2409.18523.\n\n\n\n\n\nFigureÂ 1: Overview of TokenCache\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/slides.html#tokencache",
    "href": "posts/papers/token_cache_2409_18523/slides.html#tokencache",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Î¸} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Î¸}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Î¸}?\n\nInstead of predicing binary importance [0,1], use interpolation to â€œsuperposeâ€ pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} â¦¿ f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) â¦¿ f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\nğŸ” Optimization Objective\n\n\n\nâ„’_{\\text{mse}} = ğ”¼_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [â€– z^{t}_{L+1} - \\hat{z}^{t}_{L+1}â€–^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/slides.html#cache-predictor-visualized",
    "href": "posts/papers/token_cache_2409_18523/slides.html#cache-predictor-visualized",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/token_cache_2409_18523/slides.html#bibliography",
    "href": "posts/papers/token_cache_2409_18523/slides.html#bibliography",
    "title": "Token Caching for Diffusion Transformer Acceleration arxiv:2409.18523",
    "section": "ğŸ“‘ Bibliography",
    "text": "ğŸ“‘ Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. â€œToken Caching for Diffusion Transformer Acceleration.â€ https://arxiv.org/abs/2409.18523.\n\n\n\n\n\n\n\n\n\nFigureÂ 1: Overview of TokenCache\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/index.html#tokencache",
    "href": "posts/papers/ToCa/index.html#tokencache",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Î¸} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Î¸}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Î¸}?\n\nInstead of predicing binary importance [0,1], use interpolation to â€œsuperposeâ€ pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} â¦¿ f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) â¦¿ f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\n\n\nğŸ” Optimization Objective\n\n\n\n\nâ„’_{\\text{mse}} = ğ”¼_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [â€– z^{t}_{L+1} - \\hat{z}^{t}_{L+1}â€–^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/ToCa/index.html#cache-predictor-visualized",
    "href": "posts/papers/ToCa/index.html#cache-predictor-visualized",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\n\n\n\n\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/index.html#bibliography",
    "href": "posts/papers/ToCa/index.html#bibliography",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "ğŸ“‘ Bibliography",
    "text": "ğŸ“‘ Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. â€œToken Caching for Diffusion Transformer Acceleration.â€ https://arxiv.org/abs/2409.18523.\n\n\n\n\n\nFigureÂ 1: Overview of TokenCache\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/slides.html#tokencache",
    "href": "posts/papers/ToCa/slides.html#tokencache",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "TokenCache",
    "text": "TokenCache\n\n\n\nUse a small learnable network g_{Î¸} dubbed Cache Predictor Lou et al. (2024) to predict the importance of the tokens w^{t}_{l} = g_{Î¸}(l, t) \\in \\mathbf{R}^{n}, and prune the tokens based on their relative importance.\nHow to learn g_{Î¸}?\n\nInstead of predicing binary importance [0,1], use interpolation to â€œsuperposeâ€ pruned and non-pruned token states\nInterpolated states are gerenated via: \\hat{z}^{t}_{l+1} = z^{t}_{l} + \\hat{f}_{l}(z^{t}_{l})\n\n\\hat{f}(z^{t}_{l}) = \\textcolor{#800080}{w^{t}_{l} â¦¿ f_{l}(z^{t}_{l})} + \\textcolor{#008080}{(1 - w^{t}_{l}) â¦¿ f_{l}(z^{t+1}_{l})}    where diffusion transformer contains total number of L network blocks, with a total of T inference timesteps and z^{t}_{l} \\in \\mathbf{R}^{n \\times d} denotes input to block f_{l} at timestep t\n\n\n\n\n\nğŸ” Optimization Objective\n\n\n\nâ„’_{\\text{mse}} = ğ”¼_{t, z^{t}_{L+1}, \\hat{z}^{t}_{L+1}} [â€– z^{t}_{L+1} - \\hat{z}^{t}_{L+1}â€–^{2}_{2}]"
  },
  {
    "objectID": "posts/papers/ToCa/slides.html#cache-predictor-visualized",
    "href": "posts/papers/ToCa/slides.html#cache-predictor-visualized",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "Cache Predictor (Visualized)",
    "text": "Cache Predictor (Visualized)\n\n\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "posts/papers/ToCa/slides.html#bibliography",
    "href": "posts/papers/ToCa/slides.html#bibliography",
    "title": "ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHINGarXiv:2410.05317",
    "section": "ğŸ“‘ Bibliography",
    "text": "ğŸ“‘ Bibliography\n\n\nLou, Jinming, Wenyang Luo, Yufan Liu, Bing Li, Xinmiao Ding, Weiming Hu, Jiajiong Cao, Yuming Li, and Chenguang Ma. 2024. â€œToken Caching for Diffusion Transformer Acceleration.â€ https://arxiv.org/abs/2409.18523.\n\n\n\n\n\n\n\n\n\nFigureÂ 1: Overview of TokenCache\nFigureÂ 2: Overview of Cache Predictor"
  },
  {
    "objectID": "index.html#about-meinspiration",
    "href": "index.html#about-meinspiration",
    "title": "",
    "section": "ğŸ§‘ğŸ»â€ğŸ’» About Me1",
    "text": "ğŸ§‘ğŸ»â€ğŸ’» About Me1\n\nğŸ’» currently a computer science graduate student @ (UT Austin).\n\n\n\nğŸš€ generatly interested in application of AI/ML specifically in domains of Generative AI (Diffusion Models), 3D Vision, & Multi-Modality2\n\n\n\n\n\nWebsite inspiration and code base ported from Sam Foremanâ€™s Personal Website. Huge thanks to Sam ğŸ!â†©ï¸\nIf this sounds like something youâ€™d be interested in doing, please feel free to reach out to me!â†©ï¸"
  },
  {
    "objectID": "posts/index.html#reading-list---arxiv-papers",
    "href": "posts/index.html#reading-list---arxiv-papers",
    "title": "ğŸ“¬ Posts",
    "section": "ğŸ—ï¸Reading List - arXiv papers",
    "text": "ğŸ—ï¸Reading List - arXiv papers\n\nView Reading List"
  },
  {
    "objectID": "posts/papers/index.html#reading-list---arxiv-papers",
    "href": "posts/papers/index.html#reading-list---arxiv-papers",
    "title": "ğŸ“¬ Posts",
    "section": "ğŸš€ Reading List - arXiv papers",
    "text": "ğŸš€ Reading List - arXiv papers"
  }
]