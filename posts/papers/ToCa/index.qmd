---
lightbox: auto
title: " ACCELERATING DIFFUSION TRANSFORMERS WITH TOKEN-WISE FEATURE CACHING[`arXiv:2410.05317`](https://arxiv.org/abs/2410.05317)"
date: 2024-10-16
date-modified: 2024-10-16
bibliography: ../../../references.bib
appendix-cite-as: display
category: reading-list
keywords:
  - Diffusion models
  - Token-Feature caching
  - Score-based caching
  - Model ccceleration
# favicon: "./assets/favicon.svg"
# date: 2023-07-31
# image: "assets/thumbnail.png"
# footer: "{{< fa brands github >}} [saforem2.github.io/lattice23](https://saforem2.github.io/lattice23)"
# twitter-card:
#   site: "saforem2"
#   creator: "saforem2"
#   image: "./assets/thumbnail.png"
#   title: "MLMC: Machine Learning Monte Carlo"
#   description: "MLMC: Machine Learning Monte Carlo for Lattice Gauge Theory"
# open-graph:
#   image: "./assets/thumbnail.png"
#   title: "MLMC: Machine Learning Monte Carlo"
#   description: "MLMC: Machine Learning Monte Carlo for Lattice Gauge Theory"
author:
  name: Muhammad Muaz
  # url: https://cs.utexas.edu/~mmuaz/
  # orcid: 0000-0002-9981-0876
  email: m.muaz@utexas.edu
  # affiliation: University of Texas at Austin
  # affiliation-url: https://alcf.anl.gov/about/people/sam-foreman

callout-style: simple
format:
  html: 
    page-layout: full
  #   theme:
  #       dark: cyborg
  #       light: cosmo
  revealjs:
    # slide-url: https://cs.utexas.edu/~mmuaz/docs/posts/papers/token_cache_2409_18523/slides.html
    output-file: "slides.html"
    title-block-style: none
    slide-number: c
    title-slide-style: default
    title-slide-attributes:
        data-background-iframe: https://emilhvitfeldt.github.io/quarto-iframe-examples/colored-particles/index.html
        data-background-size: contain
        # data-background-color: white
        # background-color: white
    chalkboard: true
    auto-animate: true
    reference-location: section
    touch: true
    pause: true
    footnotes-hover: true
    citations-hover: true
    preview-links: auto
    controls-tutorial: true
    controls: true
    # logo: "https://raw.githubusercontent.com/saforem2/anl-job-talk/main/docs/assets/anl.svg"
    progress: true
    # theme: [../../../css/light.scss]
    callot-style: simple
    default-image-extension: svg
    code-overflow: scroll
    html-math-method: katex
    fig-align: center
    self-contained: false
    embed-resources: false
    self-contained-math: false
    center: true
    highlight-style: "atom-one"
    code-line-numbers: true
    mermaid:
      theme: neutral
    # theme: [../../../css/light.scss]
    # css: [../../../css/common.scss, ../../../css/callouts.css]
    css:
      # - css/default.css
      - ../../../css/common.scss
      # - /Users/muhammadmuaz/Documents/GitHub/saforem2-personal-site/css/custom.css
    theme:
      # - light:
      - white
      # - /Users/muhammadmuaz/Documents/GitHub/saforem2-personal-site/css/title-slide-template.scss
      # - /Users/muhammadmuaz/Documents/GitHub/saforem2-personal-site/css/reveal/reveal.scss
      - ../../../css/common.scss
      - ../../../css/light.scss
      - ../../../css/syntax-light.scss
      - ../../../css/callout-cards.scss
      # - dark:
      #   - black
      #   - ./title-fancy/title-slide-template.scss
      #   - ../../css/reveal/reveal.scss
      #   - ../../css/common.scss
      #   - ../../css/dark.scss
      #   - ../../css/syntax-dark.scss
      #   - ../../css/callout-cards.scss
    # gfm:
#   output-file: "lattice23.md"
---

# Overview

1. [Motivation & Key Contributions](#sec-contributions)
2. [TokenCache](#sec-tokencache)
3. [Two-Phase Round Robin Timestep Schedule](#sec-tppr)
4. [Experimental Results](#sec-exp)
5. [Limitations](#sec-limitations)
6. [References](#sec-ref)

# Motivation & Key Contributions {#sec-contributions style="font-size:0.9em;text-align: left"}

- Motivation:
  - Diffusion generative models are slow because of high computational cost, arising from:
    - **Quadratic computational complexity** of attention mechanisms 
    - Multi-step inference

&nbsp;<br>

- How to reduce this computational bottleneck?
  - Reduce redundant computations among tokens across inference steps

&nbsp;<br>

- Key Contributions:
  - *Which [tokens]{.red-text}*  should be **pruned** to eliminate redundancy?
  - *Which [blocks]{.green-text}* should be **targeted** to prune the tokens?
  - *Which [timesteps]{.purple-text}* should be applied caching?


# Token Cache (System Overview) {#sec-tokencache}

::: {#fig-token-cache}

![](./assets/token-cache.png){.r-stretch}

Overview of TokenCache
:::


## TokenCache { .centeredslide .smaller}

:::: {.columns}

::: {style="text-align:left;font-size:0.9em"}

- Use a small learnable network $g_{Œ∏}$ dubbed **Cache Predictor** @lou2024tokencachingdiffusiontransformer to predict the importance of the tokens $w^{t}_{l} = g_{Œ∏}(l, t) \in \mathbf{R}^{n}$, and prune the tokens based on their relative importance.

- How to learn $g_{Œ∏}$?
  - Instead of predicing binary importance $[0,1]$, use interpolation to "superpose" pruned and non-pruned token states
  - Interpolated states are gerenated via: 
  $$\hat{z}^{t}_{l+1} = z^{t}_{l} + \hat{f}_{l}(z^{t}_{l})$$

  $$\hat{f}(z^{t}_{l}) = \textcolor{#800080}{w^{t}_{l} ‚¶ø f_{l}(z^{t}_{l})} + \textcolor{#008080}{(1 - w^{t}_{l}) ‚¶ø f_{l}(z^{t+1}_{l})}   $$
    where diffusion transformer contains total number of $L$ network blocks, with a total of $T$ inference timesteps and $z^{t}_{l} \in \mathbf{R}^{n \times d}$ denotes input to block $f_{l}$ at timestep $t$ 

::: {.callout-important icon=false title="üîç Optimization Objective" style="text-align:center;font-size:1.5em;width=100%"}

$$
‚Ñí_{\text{mse}} = ùîº_{t, z^{t}_{L+1}, \hat{z}^{t}_{L+1}} [‚Äñ z^{t}_{L+1} - \hat{z}^{t}_{L+1}‚Äñ^{2}_{2}]
$$

:::



:::

::::

## Cache Predictor (Visualized) {.centeredslide}

::: {#fig-cache-pred}

![](./assets/cache-predictor.png){.r-stretch}

Overview of Cache Predictor
:::

# Two-Phase Round Robin Timestep Scheduler {#sec-tppr style="font-size:0.9em"}
- Token Caching inherintly adds errors into the **sampling**trajectory.

- How to mitigate errors?
  - Perform independent (no-cache) steps (I-Steps) after $K$ prediction/caching steps (P-Steps)
  - $K$ = ***Caching Interval***

- How to choose $K$?
  - Token correlations vary across timesteps.
    - Higher correlations among tokens across **early timesteps**
    - Lower correlations among tokens across **later timesteps**

::: {font-size:9em}
**TPPR**
:::
  - Phase-$1$ employs larger *caching interval* $K_1$
  - Phase-$2$ employs smaller *caching interval* $K_2$

# Experimental Results {#sec-exp .centeredslide .scrollable}

:::{layout-nrows=2}
![DiT-XL/2](./assets/dit.png){height="300" .r-stretch}
![MDT](./assets/mdt.png){height="300" .r-stretch}
:::

# References {#sec-ref visibility="hidden"}

## üìë Bibliography

::: {#refs}
:::





